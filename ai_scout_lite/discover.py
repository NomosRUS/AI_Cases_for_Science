"""Utilities for discovering information about an organisation.

The module performs five operations:
1. Find the official web site of an organisation by name.
2. Extract key facts from the official web site: activities, 2022-2025 results,
   research directions and partners.
3. Search the internet and collect the same information from public sources.
4. Store data from steps 2 and 3 in separate JSON files.
5. Save human readable summaries from steps 2 and 3 in two txt files.
"""



from __future__ import annotations
import itertools
import textwrap
from collections import OrderedDict

from rich import box, table
from readability import Document
from bs4 import BeautifulSoup as BS

from urllib.parse import urljoin, urlparse
import requests, trafilatura, time, random

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager

from urllib.parse import quote_plus
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException
from openai import OpenAI

from fake_useragent import UserAgent
import json
import logging
import os
import urllib.parse
from dataclasses import dataclass, asdict, field
from pathlib import Path

import re, urllib.parse, tldextract, requests
from transliterate import translit
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import requests_cache
import trafilatura
from .utils import extract_json
from typing import Sequence

# cache web requests to speed up repeated runs
requests_cache.install_cache("ai_scout_cache")

import time
import random
from typing import List
from duckduckgo_search import DDGS
from duckduckgo_search.exceptions import DuckDuckGoSearchException
from rich.console import Console

if os.getenv("OPENAI_API_KEY"):
    print("OPENAI_API_KEY Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ°")
else:
    print("ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ OPENAI_API_KEY Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ°")

console = Console()

# Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Firefox UA (Ğ¸ÑĞ½ÑŒ-2025)
FIREFOX_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) "
    "Gecko/20100101 Firefox/126.0"
)
HEADERS = {
    "User-Agent": FIREFOX_UA,
    "Accept":
        "text/html,application/xhtml+xml,"
        "application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "ru,en;q=0.9",
    "DNT": "1",
}

OUTPUT_ROOT = Path("output")
GOOD_TLDS = {"ru", "su", "org", "edu", "ac", "science", "tech"}
_MAX_RETRIES = 3          # ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ ÑĞ´Ğ°Ñ‚ÑŒÑÑ
_BASE_SLEEP  = 2          # Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° (ÑĞµĞº)


@dataclass
class OrgInfo:
    """Structured information about an organisation."""

    science:     List[str] = field(default_factory=list)    # Ğ½Ğ¾Ğ²Ñ‹Ğµ Ğ¿Ğ¾Ğ»Ñ
    activities:  List[str] = field(default_factory=list)
    results:     List[str] = field(default_factory=list)
    commercial:  List[str] = field(default_factory=list)    # NEW
    partners:    List[str] = field(default_factory=list)


PROMPT_INFO = """
Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ Ğ¸ Ğ¸Ğ·ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑƒÑĞ¿ĞµÑ…Ğ¾Ğ² Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ñ‚ĞµĞºÑÑ‚ {text} Ğ¸
Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸ (Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾, Ğ½Ğ¾ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€ĞµĞ´Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¸):
1. Ñ€ĞµÑˆĞ°ĞµĞ¼Ñ‹Ğµ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ĞµĞ¹ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ (Ñ Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€Ğ°Ğ¼Ğ¸ ĞµÑĞ»Ğ¸ ĞµÑÑ‚ÑŒ), 
2. Ğ´Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ (Ğ½Ğ°Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€, Ğ² Ğ¿Ñ€Ğ¾Ğ¸Ğ·Ğ²Ğ¾Ğ´ÑÑ‚Ğ²ĞµĞ½Ğ½Ğ¾Ğ¹ Ğ¸Ğ»Ğ¸ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ‡ĞµÑĞºĞ¾Ğ¹ ÑÑ„ĞµÑ€Ğµ),
3. Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ° 2024-2025 Ğ³Ğ¾Ğ´Ñ‹ (Ğ½Ğµ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ Ğ² Ğ¿Ğ¾ĞºĞ°Ğ·Ğ°Ñ‚ĞµĞ»ÑÑ…, Ğ½Ğ¾ Ğ¸ ĞºĞ°Ğº Ñ€ĞµÑˆĞµĞ½Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ·Ğ°Ğ´Ğ°Ñ‡Ğ¸),
4. Ğ¾Ğ¿Ñ‹Ñ‚ ĞºĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² (Ñ ĞºĞ°ĞºĞ¸Ğ¼Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸ÑĞ¼Ğ¸ Ğ¸ Ñ‡Ñ‚Ğ¾ Ğ¸Ğ¼ĞµĞ½Ğ½Ğ¾ Ğ¿Ñ€Ğ¸Ğ½Ğ¾ÑĞ¸Ñ‚ Ğ¿Ñ€Ğ¸Ğ±Ñ‹Ğ»ÑŒ),
5. ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ğµ Ğ¸Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ñ‹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ğ¸Ğ· Ñ€Ğ°Ğ·Ğ»Ğ¸Ñ‡Ğ½Ñ‹Ñ… Ğ¾Ñ‚Ñ€Ğ°ÑĞ»ĞµĞ¹ (Ñ ÑƒĞºĞ°Ğ·Ğ°Ğ½Ğ¸Ğ°ĞµĞ¼ ĞºĞ¾Ğ½ĞºÑ€ĞµÑ‚Ğ½Ñ‹Ñ… Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¹).
Ğ’ĞµÑ€Ğ½Ğ¸ **ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ JSON**:
{{"science": [...], "activities": [...], "results": [...], "commercial": [...], "partners": [...]}}
"""

ORG_INFO_SCHEMA = {
    "name": "extract_org_info",
    "description": "Return structured info about a research institute.",
    "parameters": {
        "type": "object",
        "properties": {
            "science":    {"type": "array", "items": {"type": "string"}},
            "activities": {"type": "array", "items": {"type": "string"}},
            "results":    {"type": "array", "items": {"type": "string"}},
            "commercial": {"type": "array", "items": {"type": "string"}},
            "partners":   {"type": "array", "items": {"type": "string"}}
        },
        "required": ["science", "activities", "results", "partners"]
    }
}
# ---------------------------------------------------------------------------
# generic helpers
# ---------------------------------------------------------------------------


def search_duckduckgo(query: str, max_results: int = 10) -> List[str]:
    """DuckDuckGo search with Firefox UA, back-off and verbose logging."""
    console.print(f"[cyan]â†’ DuckDuckGo query:[/] {query}")

    for attempt in range(3):                    # â‰¤ 3 Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸
        try:
            with DDGS(headers=HEADERS, timeout=15) as ddgs:
                hits = [
                    r["href"] for r in ddgs.text(query, max_results=max_results)
                    if r.get("href")
                ]

            if hits:
                console.print(
                    "[green]âœ” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:[/]\n  " + "\n  ".join(hits[:2])
                )
            else:
                console.print("[yellow]âš  Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾[/]")

            return hits

        except DuckDuckGoSearchException as err:
            wait = (2 ** attempt) + random.uniform(0, 1.2)
            console.print(
                f"[yellow]âš  Rate-limit:[/] {err}. "
                f"ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· {wait:0.1f} Ñ."
            )
            time.sleep(wait)

    console.print("[red]âŒ DuckDuckGo: Ğ²ÑĞµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ñ‹[/]")
    return []

def ddg_first_links_firefox(query: str, n: int = 3) -> list[str]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ n ÑÑÑ‹Ğ»Ğ¾Ğº DuckDuckGo Ñ‡ĞµÑ€ĞµĞ· headless-Firefox.
    â€¢ ĞĞµ ĞºĞ»Ğ¸ĞºĞ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ; ÑÑ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ URL Ğ²Ğ¸Ğ´Ğ° `/?q=...&ia=web`.
    â€¢ Ğ–Ğ´Ñ‘Ñ‚ Ğ´Ğ¾ 7 Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ±ĞµÑ€Ñ‘Ñ‚ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¿Ğ¾ CSS `.result__a`.
    """
    console.print(f"[cyan]â†’ Firefox DDG query:[/] {query}")

    options = Options()
    options.headless = True
    driver = webdriver.Firefox(
        service=Service(GeckoDriverManager().install()),
        options=options,
    )

    try:
        url = (
            "https://duckduckgo.com/?q="
            + quote_plus(query)
            + "&ia=web&kl=ru-ru"
        )
        driver.get(url)

        try:
            WebDriverWait(driver, 12).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a.result__a"))
            )
        except TimeoutException:
            console.print("[yellow]âš  DDG: Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ¿Ğ¾ÑĞ²Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ° 12 Ñ[/]")
            return []

        links = driver.find_elements(By.CSS_SELECTOR, "a.result__a")[: n]
        hrefs = [link.get_attribute("href") for link in links]

        if hrefs:
            console.print(
                "[green]âœ” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:[/]\n  " + "\n  ".join(hrefs)
            )
        else:
            console.print("[yellow]âš  Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾[/]")

        return hrefs

    finally:
        driver.quit()


def fetch_text(url: str) -> str:
    """Download and clean page text."""

    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            return trafilatura.extract(downloaded) or ""
    except Exception as exc:  # noqa: BLE001
        logging.warning("Failed to fetch %s: %s", url, exc)
    return ""


# ---------------------------------------------------------------------------
# information extraction
# ---------------------------------------------------------------------------




# ---------------------------------------------------------------------------
# official web site
# ---------------------------------------------------------------------------

def _clean_name(name: str) -> str:
    _BAD_PREFIXES = (
        "Ğ¤Ğ•Ğ”Ğ•Ğ ĞĞ›Ğ¬ĞĞĞ• Ğ“ĞĞ¡Ğ£Ğ”ĞĞ Ğ¡Ğ¢Ğ’Ğ•ĞĞĞĞ• Ğ‘Ğ®Ğ”Ğ–Ğ•Ğ¢ĞĞĞ• Ğ£Ğ§Ğ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ• ĞĞĞ£ĞšĞ˜",
        "Ğ¤Ğ•Ğ”Ğ•Ğ ĞĞ›Ğ¬ĞĞĞ• Ğ“ĞĞ¡Ğ£Ğ”ĞĞ Ğ¡Ğ¢Ğ’Ğ•ĞĞĞĞ• Ğ‘Ğ®Ğ”Ğ–Ğ•Ğ¢ĞĞĞ• ĞĞĞ£Ğ§ĞĞĞ• Ğ£Ğ§Ğ Ğ•Ğ–Ğ”Ğ•ĞĞ˜Ğ•",
        "Ğ¤Ğ“Ğ‘Ğ£",
        "Ğ¤Ğ˜Ğ¦",
        "Ğ¤Ğ•Ğ”Ğ•Ğ ĞĞ›Ğ¬ĞĞ«Ğ™ Ğ˜Ğ¡Ğ¡Ğ›Ğ•Ğ”ĞĞ’ĞĞ¢Ğ•Ğ›Ğ¬Ğ¡ĞšĞ˜Ğ™ Ğ¦Ğ•ĞĞ¢Ğ ",
        "Ğ¤Ğ“Ğ£ĞŸ",
        "ĞĞ",
    )
    """Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑ€.Ñ„Ğ¾Ñ€Ğ¼Ñ‹ / ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Â«ÑÑƒÑ‚ÑŒÂ»."""
    txt = name.strip(" Â«Â»\"")
    for bad in _BAD_PREFIXES:
        if txt.upper().startswith(bad):
            txt = txt[len(bad):].lstrip(" ,")
    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ Â«Ğ¸Ğ¼. Ğ¤. Ğ˜. Ğ.Â» Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾ ĞºĞ°Ğº Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğ¹ ĞºÑƒÑĞ¾Ğº, Ğ½Ğµ Ğ²ÑÑ‘ Ğ¿Ğ¾Ğ´Ñ€ÑĞ´
    txt = re.sub(r"\bĞ¸Ğ¼\.\s+", "", txt, flags=re.I).strip()
    # ÑƒĞ±Ğ¸Ñ€Ğ°ĞµĞ¼ ĞºÑ€Ğ°Ğ¹Ğ½Ğ¸Ğµ ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸, ĞµÑĞ»Ğ¸ Ğ¾ÑÑ‚Ğ°Ğ»Ğ¸ÑÑŒ
    return txt.strip(" Â«Â»\"")


#def _looks_like_official(url: str, clean: str, abbr: str) -> bool:
    # """Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°: Ğ´Ğ¾Ğ¼ĞµĞ½ Ğ½Ğµ wiki/ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº + ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ
    # Ğ¸Ğ»Ğ¸ Ğ»ÑĞ±ÑƒÑ Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ."""
    # host = urllib.parse.urlparse(url).netloc.lower()
    #
    # if host.startswith("ru.wikipedia.org") or host.endswith(".wikipedia.org"):
    #     return False
    #
    # # Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ (Institute â†’ institute, Ğ˜Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚ â†’ institut)
    # first_word = translit(clean.split()[0], 'ru', reversed=True).lower()
    #
    # return (
    #     first_word in host
    #     or abbr.lower() in host              # Ğ ĞĞ â†’ ran, Ğ˜ĞĞ¥ â†’ inh Ğ¸ Ñ‚.Ğ¿.
    #     or "ras." in host                    # Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ ĞĞ
    # )
SCIENCE_ZONES = (
    "ras.ru", "ran.ru", "nsc.ru",        # Ğ¡Ğ¸Ğ±Ğ¸Ñ€ÑĞºĞ¾Ğµ
    "sbras.ru", "febras.ru", "ural.ru",  # Ğ¾Ñ‚Ğ´ĞµĞ»ĞµĞ½Ğ¸Ñ Ğ ĞĞ
    "iacp.dvo.ru", "ru/science",         # Ğ¿Ñ€Ğ¸Ğ¼ĞµÑ€
)

def _looks_like_official(url: str, clean: str, abbr: str) -> bool:
    host = urllib.parse.urlparse(url).netloc.lower()

    # 0. Ğ¡Ñ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚ÑĞµĞ¸Ğ²Ğ°ĞµĞ¼ Ğ’Ğ¸ĞºĞ¸Ğ¿ĞµĞ´Ğ¸Ñ / ÑĞ»Ğ¾Ğ²Ğ°Ñ€Ğ¸
    if ".wikipedia.org" in host or host.endswith(".academic.ru"):
        return False

    # 1. Ğ¢Ñ€Ğ°Ğ½ÑĞ»Ğ¸Ñ‚ Ğ¿ĞµÑ€Ğ²Ğ¾Ğ³Ğ¾ Ğ·Ğ½Ğ°Ñ‡Ğ¸Ğ¼Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°
    first_word = translit(clean.split()[0], "ru", reversed=True).lower()

    # 2. ĞŸĞ¾Ğ´ÑÑ‚Ñ€Ğ¾ĞºĞ¸ Ğ¸Ğ· Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ñ… ÑĞ»Ğ¾Ğ² (â‰¥6)
    stem_hits = False
    for word in clean.split():
        if len(word) >= 6:
            stem = translit(word[:5], "ru", reversed=True).lower()
            if stem in host:
                stem_hits = True
                break

    # 3. Ğ¡Ğ´Ğ²Ğ¸Ğ³Ğ°ÑÑ‰ĞµĞµÑÑ Ğ¾ĞºĞ½Ğ¾ 3-ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹
    abbr_hits = any(abbr.lower()[i : i + 3] in host for i in range(len(abbr) - 2))

    # 4. ĞĞ°ÑƒÑ‡Ğ½Ğ°Ñ Â«Ğ·Ğ¾Ğ½Ğ°Â»
    zone_hit = host.endswith(SCIENCE_ZONES)

    return (
        first_word in host
        or abbr.lower() in host         # Ğ¿Ğ¾Ğ»Ğ½Ğ¾Ğµ ÑĞ¾Ğ²Ğ¿Ğ°Ğ´ĞµĞ½Ğ¸Ğµ Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹
        or abbr_hits                    # â‰¥3 Ğ¿Ğ¾Ğ´Ñ€ÑĞ´ Ğ±ÑƒĞºĞ² Ğ¸Ğ· Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñ‹
        or stem_hits                    # ĞºÑƒÑĞ¾Ğº Ğ´Ğ»Ğ¸Ğ½Ğ½Ğ¾Ğ³Ğ¾ ÑĞ»Ğ¾Ğ²Ğ°
        or zone_hit                     # Ğ¿Ğ¾Ğ´Ğ´Ğ¾Ğ¼ĞµĞ½ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ³Ğ¾ ĞºĞ»Ğ°ÑÑ‚ĞµÑ€Ğ°
    )

def find_official_site(org: str) -> str:
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ URL Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ° Ğ»Ğ¸Ğ±Ğ¾ ''."""
    console.rule("[bold green]ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ°")
    clean = _clean_name(org)
    abbr  = "".join(w[0] for w in clean.split() if len(w) > 2)

    queries = [
        f"{clean} Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚",
        f"{clean} ÑĞ°Ğ¹Ñ‚ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸",
        f"{translit(clean, 'ru', reversed=True)} official website",
        f"{abbr} ÑĞ°Ğ¹Ñ‚" if len(abbr) > 3 else "",
    ]

    for q in queries:
        if not q:
            continue

        # ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢ â€” headless Firefox (Ğ±ĞµĞ· ratelimit)
        for url in ddg_first_links_firefox(q, n=4):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]âœ” Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½:[/] {url}")
                return url

        # fallback Ğ½Ğ° duckduckgo_search (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ¾ Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ¼ 202)
        for url in search_duckduckgo(q, max_results=2):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]âœ” Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½:[/] {url}")
                return url

        time.sleep(1)  # Ñ‡ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ¹

    console.print("[yellow]âš  Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
    return ""



#def extract_official_info(org: str) -> OrgInfo:
    # """
    # 1) Ğ¸Ñ‰ĞµÑ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚;
    # 2) ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚ (Ñ‡ĞµÑ€ĞµĞ· _diagnostic_download);
    # 3) Ğ¿Ğ¸ÑˆĞµÑ‚ raw-Ñ‚ĞµĞºÑÑ‚ Ğ² output/<org>/site_info.txt, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ½Ğµ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹;
    # 4) Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ OrgInfo (Ğ¸Ğ»Ğ¸ Â«Ğ¿ÑƒÑÑ‚ÑƒÑÂ», ĞµÑĞ»Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ° Ğ½ĞµÑ‚).
    # """
    # url = find_official_site(org)
    # if not url:
    #     logging.warning("Official site for %s not found", org)
    #     return OrgInfo([], [], [], [])
    #
    # console.print(f"[bold]Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚: {url}")
    # text = _diagnostic_download(url)            # â† Ğ½Ğ¾Ğ²Ñ‹Ğ¹ helper
    #
    # # â”€â”€ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾, ĞµÑĞ»Ğ¸ Ñ…Ğ¾Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # if text and len(text) >= 500:
    #     org_dir = OUTPUT_ROOT / org.replace(" ", "_")
    #     org_dir.mkdir(parents=True, exist_ok=True)
    #     (org_dir / "site_info.txt").write_text(text, encoding="utf-8")
    #     console.print(f"[green]ğŸ“ site_info.txt Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½ ({len(text)} ÑĞ¸Ğ¼Ğ².)")
    # else:
    #     console.print("[yellow]âš  Ñ‚ĞµĞºÑÑ‚ Ğ¿ÑƒÑÑ‚ â€” Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½")
    #
    # return _extract_info(text)

def extract_official_info(org: str, out_dir: Path) -> OrgInfo:
    """
    1) ĞĞ°Ñ…Ğ¾Ğ´Ğ¸Ñ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚.
    2) ĞšÑ€Ğ°ÑƒĞ»Ğ¸Ñ‚ Ğ³Ğ»Ğ°Ğ²Ğ½ÑƒÑ + ÑÑÑ‹Ğ»ĞºĞ¸ 1-Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ (crawl_one_level).
    3) Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½ÑĞµÑ‚ ÑÑ‹Ñ€Ğ¾Ğ¹ Ñ‚ĞµĞºÑÑ‚ Ğ² site_info.txt.
    4) ĞŸÑ€Ğ¾Ğ³Ğ¾Ğ½ÑĞµÑ‚ LLM-ÑĞºÑÑ‚Ñ€Ğ°ĞºÑ†Ğ¸Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ OrgInfo.
    """
    url = find_official_site(org)
    if not url:
        console.print("[yellow]âš  ĞÑ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
        return OrgInfo()                       # Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ dataclass

    console.print(f"[bold]ğŸŒ ĞšÑ€Ğ°ÑƒĞ»Ñ ÑĞ°Ğ¹Ñ‚ (1 ÑƒÑ€Ğ¾Ğ²ĞµĞ½ÑŒ): {url}")
    text = crawl_one_level(url)

    if not text:
        console.print("[yellow]âš  ĞĞµÑ‚ Ğ¿Ñ€Ğ¸Ğ³Ğ¾Ğ´Ğ½Ğ¾Ğ³Ğ¾ Ñ‚ĞµĞºÑÑ‚Ğ°")
        return OrgInfo()

    (out_dir / "site_info.txt").write_text(text, encoding="utf-8")
    console.print(f"[green]ğŸ“ site_info.txt Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½ ({len(text)} ÑĞ¸Ğ¼Ğ².)")

    return _extract_info(text)                 # Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ chunk-Ñ€ĞµĞ¶Ğ¸Ğ¼
# ---------------------------------------------------------------------------
# internet search
# ---------------------------------------------------------------------------

def gather_internet_info(org: str, max_results: int = 10) -> OrgInfo:
    """Search the web for public information about the organisation."""
    console.print("Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸")

    texts: List[str] = []
    query = f"{org} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ¿Ğ°Ñ€Ñ‚Ğ½ĞµÑ€Ñ‹ Ğ¸ÑÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ğ½Ğ¸Ñ"
    for url in search_duckduckgo(query, max_results=max_results):
        txt = fetch_text(url)
        if txt:
            texts.append(txt)
    return _extract_info("\n".join(texts))


# ---------------------------------------------------------------------------
# saving helpers
# ---------------------------------------------------------------------------

def save_json(info: OrgInfo, path: Path) -> None:
    path.write_text(json.dumps(asdict(info), ensure_ascii=False, indent=2), encoding="utf-8")
    console.print("ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»")

def info_as_text(info: OrgInfo) -> str:
    lines = ["# ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ¿Ñ€Ğ¾Ğ±Ğ»ĞµĞ¼Ñ‹ (science):"]
    lines += [f"- {s}" for s in info.science]

    lines.append("\n# Ğ”Ñ€ÑƒĞ³Ğ¸Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸:")
    lines += [f"- {a}" for a in info.activities]

    lines.append("\n# ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 2024-2025:")
    lines += [f"- {r}" for r in info.results]

    lines.append("\n# ĞšĞ¾Ğ¼Ğ¼ĞµÑ€Ñ†Ğ¸Ğ°Ğ»Ğ¸Ğ·Ğ°Ñ†Ğ¸Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ñ… Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²:")
    lines += [f"- {c}" for c in info.commercial]

    lines.append("\n# Ğ˜Ğ½Ğ´ÑƒÑÑ‚Ñ€Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ñ‹:")
    lines += [f"- {p}" for p in info.partners]

    return "\n".join(lines)


def save_txt(info: OrgInfo, path: Path) -> None:
    path.write_text(info_as_text(info), encoding="utf-8")


# ---------------------------------------------------------------------------
# public API
# ---------------------------------------------------------------------------

def discover_org(org: str, output_dir: Path) -> None:
    """Run discovery pipeline for the organisation."""
    console.print("Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸")
    output_dir.mkdir(parents=True, exist_ok=True)

    site_info = extract_official_info(org, output_dir)
    web_info = gather_internet_info(org)

    save_json(site_info, output_dir / "site_info.json")
    save_json(web_info, output_dir / "internet_info.json")

    save_txt(site_info, output_dir / "site_info.txt")
    save_txt(web_info, output_dir / "internet_info.txt")

def _diagnostic_download(url: str) -> str:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ URL, Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³Ğ¸, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ ('' ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚)."""
    console.rule(f"[bold blue]ğŸŒ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ {url}")
    headers = {"User-Agent": FIREFOX_UA}

    # â”€â”€ 1. HTTP GET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        r = requests.get(url, headers=headers, timeout=20)
        console.print(f"Status: {r.status_code}, bytes: {len(r.content)}")
        r.raise_for_status()
    except requests.RequestException as err:
        console.print(f"[red]HTTP error:[/] {err}")
        return ""

    # â”€â”€ 2. ĞšĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    enc_before = r.encoding or "None"
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    console.print(f"Encoding: {enc_before} â†’ {r.encoding}")

    html = r.text

    # â”€â”€ 3. Trafilatura â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.print("â€¢ Trafilatura.extract() â€¦")
    text = trafilatura.extract(
        html,
        include_images=False,
        include_tables=False,
        no_fallback=False,
        target_language="ru",
    )
    if text:
        console.print(f"[green]âœ” Trafilatura OK:[/] {len(text)} chars")
        return text

    console.print("[yellow]Trafilatura Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ° None â€“ Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ fallback Readability")

    # â”€â”€ 4. Readability fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        doc = Document(html)
        text = BS(doc.summary(), "lxml").get_text(" ", strip=True)
        console.print(f"[green]âœ” Readability OK:[/] {len(text)} chars")
        return text
    except Exception as err:
        console.print(f"[red]Readability failed:[/] {err}")
        return ""

def crawl_one_level(
    start_url: str,
    max_pages: int = 10,
    min_len: int = 200,
    page_max_chars: int = 15_000,   # â† ĞĞĞ’ĞĞ•: Ğ¼Ğ°ĞºÑĞ¸Ğ¼ÑƒĞ¼ ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ñ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹
) -> str:
    """
    Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ³Ğ»Ğ°Ğ²Ğ½ÑƒÑ + Ğ²ÑĞµ ÑÑÑ‹Ğ»ĞºĞ¸ 1-Ğ³Ğ¾ ÑƒÑ€Ğ¾Ğ²Ğ½Ñ Ğ¸ Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚.
    â€¢ Ğ•ÑĞ»Ğ¸ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ < min_len â€” Ğ¿Ñ€Ğ¾Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñƒ.
    â€¢ Ğ•ÑĞ»Ğ¸ Ğ¾Ñ‡Ğ¸Ñ‰ĞµĞ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ > page_max_chars â€” Ğ¾Ğ±Ñ€ĞµĞ·Ğ°ĞµĞ¼ ĞµĞ³Ğ¾ Ğ´Ğ¾ page_max_chars.
    """
    domain = urlparse(start_url).netloc
    visited: set[str] = set()
    queue:   list[str] = [start_url]
    texts:   list[str] = []

    while queue and len(visited) < max_pages:
        url = queue.pop(0)
        if url in visited:
            continue
        visited.add(url)

        try:
            r = requests.get(url, headers=HEADERS, timeout=15)
            r.raise_for_status()
        except requests.RequestException:
            continue

        html = r.text
        txt = trafilatura.extract(html, target_language="ru", no_fallback=False) or ""
        if len(txt) >= min_len:
            # â”€â”€ Ğ¾Ğ³Ñ€Ğ°Ğ½Ğ¸Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ Ñ€Ğ°Ğ·Ğ¼ĞµÑ€ Ğ¾Ğ´Ğ½Ğ¾Ğ¹ ÑÑ‚Ñ€Ğ°Ğ½Ğ¸Ñ†Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if len(txt) > page_max_chars:
                txt = txt[:page_max_chars]
            texts.append(txt)

        # ÑĞ¾Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ³Ğ»ÑƒĞ±Ğ¸Ğ½Ñ‹ 1
        soup = BS(html, "lxml")
        for a in soup.find_all("a", href=True):
            link = urljoin(url, a["href"])
            if urlparse(link).netloc == domain and link not in visited:
                queue.append(link)

        time.sleep(0.5 + random.uniform(0, 0.5))

    return "\n".join(texts)

def _extract_info(text: str,
                  model: str = "gpt-4o-mini",
                  chunk: int = 30_000) -> OrgInfo:
    """
    â€¢ Ğ•ÑĞ»Ğ¸ Ñ‚ĞµĞºÑÑ‚ â‰¤ chunk â€” ĞµĞ´Ğ¸Ğ½Ğ¸Ñ‡Ğ½Ñ‹Ğ¹ Ğ²Ñ‹Ğ·Ğ¾Ğ² function-calling.
    â€¢ Ğ•ÑĞ»Ğ¸ Ğ±Ğ¾Ğ»ÑŒÑˆĞµ â€” Ñ€ĞµĞ¶ĞµĞ¼ Ğ½Ğ° ĞºÑƒÑĞºĞ¸ Ğ¸ Ğ°Ğ³Ñ€ĞµĞ³Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¾Ñ‚Ğ²ĞµÑ‚Ñ‹.
    """
    def call_llm(piece: str) -> dict:
        user_msg = PROMPT_INFO.format(text=piece)
        resp = client.chat.completions.create(
            model=model,
            messages=[
                {"role": "system",
                 "content": "Ğ¢Ñ‹ ÑĞºÑĞ¿ĞµÑ€Ñ‚ Ğ¿Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞµ. "
                            "ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ñ‚ĞµĞºÑÑ‚ Ğ¸ Ğ²Ñ‹Ğ·Ğ¾Ğ²Ğ¸ Ñ„ÑƒĞ½ĞºÑ†Ğ¸Ñ extract_org_info."},
                {"role": "user", "content": user_msg}
            ],
            functions=[ORG_INFO_SCHEMA],
            function_call={"name": "extract_org_info"},
        )
        return json.loads(resp.choices[0].message.function_call.arguments)

    client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))

    # â”€â”€ ĞºĞ¾Ñ€Ğ¾Ñ‚ĞºĞ¸Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if len(text) <= chunk:
        data = call_llm(text)
        return OrgInfo(**{k: data.get(k, []) for k in OrgInfo.__dataclass_fields__})

    # â”€â”€ Ğ´Ğ»Ğ¸Ğ½Ğ½Ñ‹Ğµ Ñ‚ĞµĞºÑÑ‚Ñ‹  â†’ chunk-map-reduce â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.print(f"[cyan]ğŸ”§ Text = {len(text):,} chars â†’ chunking")
    parts = textwrap.wrap(text, chunk)
    agg = {k: [] for k in OrgInfo.__dataclass_fields__}

    for i, part in enumerate(parts, 1):
        console.print(f"â®‘  Chunk {i}/{len(parts)} ({len(part)} chars)")
        data = call_llm(part)
        for k in agg:
            agg[k].extend(data.get(k, []))

    # Ğ´ĞµĞ´ÑƒĞ¿Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ñ Ğ¸ ÑƒÑĞµÑ‡Ñ‘Ğ½Ğ½Ñ‹Ğµ ÑĞ¿Ğ¸ÑĞºĞ¸ (â‰¤15 Ğ¿ÑƒĞ½ĞºÑ‚Ğ¾Ğ²):
    for k in agg:
        agg[k] = list(dict.fromkeys(x.strip() for x in agg[k] if x.strip()))[:15]

    return OrgInfo(**agg)