"""Utilities for discovering information about an organisation.

The module performs five operations:
1. Find the official web site of an organisation by name.
2. Extract key facts from the official web site: activities, 2022-2025 results,
   research directions and partners.
3. Search the internet and collect the same information from public sources.
4. Store data from steps 2 and 3 in separate JSON files.
5. Save human readable summaries from steps 2 and 3 in two txt files.
"""



from __future__ import annotations
import itertools


from rich import box, table
from readability import Document
from bs4 import BeautifulSoup as BS

from urllib.parse import urljoin, urlparse
import requests, trafilatura, time, random

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager

from urllib.parse import quote_plus
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException


from fake_useragent import UserAgent
import json
import logging
import os
import urllib.parse
from dataclasses import dataclass, asdict
from pathlib import Path

import re, urllib.parse, tldextract, requests
from transliterate import translit
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import requests_cache
import trafilatura
from .utils import extract_json
from typing import Sequence

# cache web requests to speed up repeated runs
requests_cache.install_cache("ai_scout_cache")

import time
import random
from typing import List
from duckduckgo_search import DDGS
from duckduckgo_search.exceptions import DuckDuckGoSearchException
from rich.console import Console

if os.getenv("OPENAI_API_KEY"):
    print("OPENAI_API_KEY –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∞")
else:
    print("–ü–µ—Ä–µ–º–µ–Ω–Ω–∞—è OPENAI_API_KEY –Ω–µ –∑–∞–¥–∞–Ω–∞")

console = Console()

# –†–µ–∞–ª—å–Ω—ã–π Firefox UA (–∏—é–Ω—å-2025)
FIREFOX_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) "
    "Gecko/20100101 Firefox/126.0"
)
HEADERS = {
    "User-Agent": FIREFOX_UA,
    "Accept":
        "text/html,application/xhtml+xml,"
        "application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "ru,en;q=0.9",
    "DNT": "1",
}

OUTPUT_ROOT = Path("output")
GOOD_TLDS = {"ru", "su", "org", "edu", "ac", "science", "tech"}
_MAX_RETRIES = 3          # —Å–∫–æ–ª—å–∫–æ —Ä–∞–∑ –ø—Ä–æ–±—É–µ–º –ø—Ä–µ–∂–¥–µ —á–µ–º —Å–¥–∞—Ç—å—Å—è
_BASE_SLEEP  = 2          # –±–∞–∑–æ–≤–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ (—Å–µ–∫)


@dataclass
class OrgInfo:
    """Structured information about an organisation."""

    activities: List[str]
    results: List[str]
    research: List[str]
    partners: List[str]


PROMPT_INFO = """
–¢—ã –∞—Å—Å–æ—Ü–∏–∏—Ä—É–µ—à—å—Å—è —Å –Ω–∞—É—á–Ω–æ–π –∞–Ω–∞–ª–∏—Ç–∏–∫–æ–π. –ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä—É–π —Ç–µ–∫—Å—Ç {text} –∏
–≤—ã–¥–µ–ª–∏:
1. –æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏,
2. –æ—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∑–∞ 2022-2025 –≥–æ–¥—ã,
3. –æ—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞—É—á–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏,
4. –∫–ª—é—á–µ–≤—ã—Ö –ø–∞—Ä—Ç–Ω—ë—Ä–æ–≤ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏.
–í–µ—Ä–Ω–∏ **—Å—Ç—Ä–æ–≥–æ JSON**:
{{"activities": [...], "results": [...], "research": [...], "partners": [...]}}
"""


# ---------------------------------------------------------------------------
# generic helpers
# ---------------------------------------------------------------------------


def search_duckduckgo(query: str, max_results: int = 2) -> List[str]:
    """DuckDuckGo search with Firefox UA, back-off and verbose logging."""
    console.print(f"[cyan]‚Üí DuckDuckGo query:[/] {query}")

    for attempt in range(3):                    # ‚â§ 3 –ø–æ–ø—ã—Ç–∫–∏
        try:
            with DDGS(headers=HEADERS, timeout=15) as ddgs:
                hits = [
                    r["href"] for r in ddgs.text(query, max_results=max_results)
                    if r.get("href")
                ]

            if hits:
                console.print(
                    "[green]‚úî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:[/]\n  " + "\n  ".join(hits[:2])
                )
            else:
                console.print("[yellow]‚ö† –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ[/]")

            return hits

        except DuckDuckGoSearchException as err:
            wait = (2 ** attempt) + random.uniform(0, 1.2)
            console.print(
                f"[yellow]‚ö† Rate-limit:[/] {err}. "
                f"–ü–æ–≤—Ç–æ—Ä —á–µ—Ä–µ–∑ {wait:0.1f} —Å."
            )
            time.sleep(wait)

    console.print("[red]‚ùå DuckDuckGo: –≤—Å–µ –ø–æ–ø—ã—Ç–∫–∏ –∏—Å—á–µ—Ä–ø–∞–Ω—ã[/]")
    return []

def ddg_first_links_firefox(query: str, n: int = 3) -> list[str]:
    """
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø–µ—Ä–≤—ã–µ n —Å—Å—ã–ª–æ–∫ DuckDuckGo —á–µ—Ä–µ–∑ headless-Firefox.
    ‚Ä¢ –ù–µ –∫–ª–∏–∫–∞–µ—Ç —Ñ–æ—Ä–º—É; —Å—Ä–∞–∑—É –æ—Ç–∫—Ä—ã–≤–∞–µ—Ç URL –≤–∏–¥–∞ `/?q=...&ia=web`.
    ‚Ä¢ –ñ–¥—ë—Ç –¥–æ 7 —Å –ø–æ—è–≤–ª–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∏ –±–µ—Ä—ë—Ç —Å—Å—ã–ª–∫–∏ –ø–æ CSS `.result__a`.
    """
    console.print(f"[cyan]‚Üí Firefox DDG query:[/] {query}")

    options = Options()
    options.headless = True
    driver = webdriver.Firefox(
        service=Service(GeckoDriverManager().install()),
        options=options,
    )

    try:
        url = (
            "https://duckduckgo.com/?q="
            + quote_plus(query)
            + "&ia=web&kl=ru-ru"
        )
        driver.get(url)

        try:
            WebDriverWait(driver, 12).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a.result__a"))
            )
        except TimeoutException:
            console.print("[yellow]‚ö† DDG: —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –Ω–µ –ø–æ—è–≤–∏–ª–∏—Å—å –∑–∞ 12 —Å[/]")
            return []

        links = driver.find_elements(By.CSS_SELECTOR, "a.result__a")[: n]
        hrefs = [link.get_attribute("href") for link in links]

        if hrefs:
            console.print(
                "[green]‚úî —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã:[/]\n  " + "\n  ".join(hrefs)
            )
        else:
            console.print("[yellow]‚ö† –Ω–∏—á–µ–≥–æ –Ω–µ –Ω–∞–π–¥–µ–Ω–æ[/]")

        return hrefs

    finally:
        driver.quit()


def fetch_text(url: str) -> str:
    """Download and clean page text."""

    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            return trafilatura.extract(downloaded) or ""
    except Exception as exc:  # noqa: BLE001
        logging.warning("Failed to fetch %s: %s", url, exc)
    return ""


# ---------------------------------------------------------------------------
# information extraction
# ---------------------------------------------------------------------------

def _extract_info(text: str, model: str = "gpt-4o-mini") -> OrgInfo:
    """
    –ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ –Ω–∞—É—á–Ω–æ–π –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏ —Å –ø–æ–º–æ—â—å—é LLM.

    Args:
        text: —Å—ã—Ä–æ–π –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç (About, –Ω–æ–≤–æ—Å—Ç–∏, –ø—É–±–ª–∏–∫–∞—Ü–∏–∏).
        model: –∏–º—è –º–æ–¥–µ–ª–∏ OpenAI Chat-Completion API (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é gpt-4o-mini).

    Returns:
        OrgInfo dataclass —Å —á–µ—Ç—ã—Ä—å–º—è —Å–ø–∏—Å–∫–∞–º–∏ —Å—Ç—Ä–æ–∫.
    """
    console.print("[bold cyan]‚è≥  –ò–∑–≤–ª–µ–∫–∞–µ–º –∫–ª—é—á–µ–≤—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –∏–∑ —Ç–µ–∫—Å—Ç–∞‚Ä¶")

    # 1. –°–æ–∑–¥–∞—ë–º —á–∞—Ç-LLM
    llm = ChatOpenAI(
        model=model,
        temperature=0,
        openai_api_key=os.getenv("OPENAI_API_KEY"),  # –±–µ–∑ –¥–µ—Ñ–æ–ª—Ç–∞-–ø–ª–∞—Ü–µ–±–æ!
    )

    # 2. –ì–æ—Ç–æ–≤–∏–º —á–∞—Ç-—à–∞–±–ª–æ–Ω
    prompt = ChatPromptTemplate.from_template(PROMPT_INFO)
    print(prompt.format(text="demo"))

    # 3. –ó–∞–ø—É—Å–∫–∞–µ–º —Ü–µ–ø–æ—á–∫—É (Prompt ‚Üí ChatOpenAI)
    chain = prompt | llm
    result = chain.invoke({"text": text[:10000]})  # ‚â§4 000 —Å–∏–º–≤–æ–ª–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏

    # 4. JSON-–ø–∞—Ä—Å–∏–Ω–≥ + —Ñ–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ dataclass
    data = extract_json(result)
    return OrgInfo(
        activities=data.get("activities", []),
        results=data.get("results", []),
        research=data.get("research", []),
        partners=data.get("partners", []),
    )

# ---------------------------------------------------------------------------
# official web site
# ---------------------------------------------------------------------------

def _clean_name(name: str) -> str:
    _BAD_PREFIXES = (
        "–§–ï–î–ï–†–ê–õ–¨–ù–û–ï –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–û–ï –ë–Æ–î–ñ–ï–¢–ù–û–ï –£–ß–†–ï–ñ–î–ï–ù–ò–ï –ù–ê–£–ö–ò",
        "–§–ï–î–ï–†–ê–õ–¨–ù–û–ï –ì–û–°–£–î–ê–†–°–¢–í–ï–ù–ù–û–ï –ë–Æ–î–ñ–ï–¢–ù–û–ï –ù–ê–£–ß–ù–û–ï –£–ß–†–ï–ñ–î–ï–ù–ò–ï",
        "–§–ì–ë–£",
        "–§–ò–¶",
        "–§–ï–î–ï–†–ê–õ–¨–ù–´–ô –ò–°–°–õ–ï–î–û–í–ê–¢–ï–õ–¨–°–ö–ò–ô –¶–ï–ù–¢–†",
        "–§–ì–£–ü",
        "–ê–û",
    )
    """–£–±–∏—Ä–∞–µ–º —é—Ä.—Ñ–æ—Ä–º—ã / –∫–∞–≤—ã—á–∫–∏, –æ—Å—Ç–∞–≤–ª—è—è ¬´—Å—É—Ç—å¬ª."""
    txt = name.strip(" ¬´¬ª\"")
    for bad in _BAD_PREFIXES:
        if txt.upper().startswith(bad):
            txt = txt[len(bad):].lstrip(" ,")
    # —É–±–∏—Ä–∞–µ–º ¬´–∏–º. –§. –ò. –û.¬ª —Ç–æ–ª—å–∫–æ –∫–∞–∫ –æ—Ç–¥–µ–ª—å–Ω—ã–π –∫—É—Å–æ–∫, –Ω–µ –≤—Å—ë –ø–æ–¥—Ä—è–¥
    txt = re.sub(r"\b–∏–º\.\s+", "", txt, flags=re.I).strip()
    # —É–±–∏—Ä–∞–µ–º –∫—Ä–∞–π–Ω–∏–µ –∫–∞–≤—ã—á–∫–∏, –µ—Å–ª–∏ –æ—Å—Ç–∞–ª–∏—Å—å
    return txt.strip(" ¬´¬ª\"")


#def _looks_like_official(url: str, clean: str, abbr: str) -> bool:
    # """–≠–≤—Ä–∏—Å—Ç–∏–∫–∞: –¥–æ–º–µ–Ω –Ω–µ wiki/—Å–ø—Ä–∞–≤–æ—á–Ω–∏–∫ + —Å–æ–¥–µ—Ä–∂–∏—Ç –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—É
    # –∏–ª–∏ –ª—é–±—É—é –ª–∞—Ç–∏–Ω–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—É—é —á–∞—Å—Ç—å –Ω–∞–∑–≤–∞–Ω–∏—è."""
    # host = urllib.parse.urlparse(url).netloc.lower()
    #
    # if host.startswith("ru.wikipedia.org") or host.endswith(".wikipedia.org"):
    #     return False
    #
    # # –ª–∞—Ç–∏–Ω–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–¥–µ—Ä–∂–∞—Ç–µ–ª—å–Ω–æ–µ —Å–ª–æ–≤–æ (Institute ‚Üí institute, –ò–Ω—Å—Ç–∏—Ç—É—Ç ‚Üí institut)
    # first_word = translit(clean.split()[0], 'ru', reversed=True).lower()
    #
    # return (
    #     first_word in host
    #     or abbr.lower() in host              # –†–ê–ù ‚Üí ran, –ò–ù–• ‚Üí inh –∏ —Ç.–ø.
    #     or "ras." in host                    # –±–æ–ª—å—à–∏–Ω—Å—Ç–≤–æ —Å–∞–π—Ç–æ–≤ –†–ê–ù
    # )
SCIENCE_ZONES = (
    "ras.ru", "ran.ru", "nsc.ru",        # –°–∏–±–∏—Ä—Å–∫–æ–µ
    "sbras.ru", "febras.ru", "ural.ru",  # –æ—Ç–¥–µ–ª–µ–Ω–∏—è –†–ê–ù
    "iacp.dvo.ru", "ru/science",         # –ø—Ä–∏–º–µ—Ä
)

def _looks_like_official(url: str, clean: str, abbr: str) -> bool:
    host = urllib.parse.urlparse(url).netloc.lower()

    # 0. –°—Ä–∞–∑—É –æ—Ç—Å–µ–∏–≤–∞–µ–º –í–∏–∫–∏–ø–µ–¥–∏—é / —Å–ª–æ–≤–∞—Ä–∏
    if ".wikipedia.org" in host or host.endswith(".academic.ru"):
        return False

    # 1. –¢—Ä–∞–Ω—Å–ª–∏—Ç –ø–µ—Ä–≤–æ–≥–æ –∑–Ω–∞—á–∏–º–æ–≥–æ —Å–ª–æ–≤–∞
    first_word = translit(clean.split()[0], "ru", reversed=True).lower()

    # 2. –ü–æ–¥—Å—Ç—Ä–æ–∫–∏ –∏–∑ –¥–ª–∏–Ω–Ω—ã—Ö —Å–ª–æ–≤ (‚â•6)
    stem_hits = False
    for word in clean.split():
        if len(word) >= 6:
            stem = translit(word[:5], "ru", reversed=True).lower()
            if stem in host:
                stem_hits = True
                break

    # 3. –°–¥–≤–∏–≥–∞—é—â–µ–µ—Å—è –æ–∫–Ω–æ 3-—Å–∏–º–≤–æ–ª–æ–≤ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
    abbr_hits = any(abbr.lower()[i : i + 3] in host for i in range(len(abbr) - 2))

    # 4. –ù–∞—É—á–Ω–∞—è ¬´–∑–æ–Ω–∞¬ª
    zone_hit = host.endswith(SCIENCE_ZONES)

    return (
        first_word in host
        or abbr.lower() in host         # –ø–æ–ª–Ω–æ–µ —Å–æ–≤–ø–∞–¥–µ–Ω–∏–µ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        or abbr_hits                    # ‚â•3 –ø–æ–¥—Ä—è–¥ –±—É–∫–≤ –∏–∑ –∞–±–±—Ä–µ–≤–∏–∞—Ç—É—Ä—ã
        or stem_hits                    # –∫—É—Å–æ–∫ –¥–ª–∏–Ω–Ω–æ–≥–æ —Å–ª–æ–≤–∞
        or zone_hit                     # –ø–æ–¥–¥–æ–º–µ–Ω –Ω–∞—É—á–Ω–æ–≥–æ –∫–ª–∞—Å—Ç–µ—Ä–∞
    )

def find_official_site(org: str) -> str:
    """–í–æ–∑–≤—Ä–∞—â–∞–µ—Ç URL –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞ –ª–∏–±–æ ''."""
    console.rule("[bold green]üîç –ü–æ–∏—Å–∫ –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω–æ–≥–æ —Å–∞–π—Ç–∞")
    clean = _clean_name(org)
    abbr  = "".join(w[0] for w in clean.split() if len(w) > 2)

    queries = [
        f"{clean} –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç",
        f"{clean} —Å–∞–π—Ç –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏",
        f"{translit(clean, 'ru', reversed=True)} official website",
        f"{abbr} —Å–∞–π—Ç" if len(abbr) > 3 else "",
    ]

    for q in queries:
        if not q:
            continue

        # –ü–†–ò–û–†–ò–¢–ï–¢ ‚Äî headless Firefox (–±–µ–∑ ratelimit)
        for url in ddg_first_links_firefox(q, n=4):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]‚úî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–∞–π–¥–µ–Ω:[/] {url}")
                return url

        # fallback –Ω–∞ duckduckgo_search (–±—ã—Å—Ç—Ä–µ–µ, –Ω–æ —Å —Ä–∏—Å–∫–æ–º 202)
        for url in search_duckduckgo(q, max_results=2):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]‚úî –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–∞–π–¥–µ–Ω:[/] {url}")
                return url

        time.sleep(1)  # —á–µ—Å—Ç–Ω–∞—è –ø–∞—É–∑–∞ –ø–µ—Ä–µ–¥ —Å–ª–µ–¥—É—é—â–µ–π —Ñ—Ä–∞–∑–æ–π

    console.print("[yellow]‚ö† –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
    return ""



#def extract_official_info(org: str) -> OrgInfo:
    # """
    # 1) –∏—â–µ—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç;
    # 2) —Å–∫–∞—á–∏–≤–∞–µ—Ç –∏ —á–∏—Å—Ç–∏—Ç —Ç–µ–∫—Å—Ç (—á–µ—Ä–µ–∑ _diagnostic_download);
    # 3) –ø–∏—à–µ—Ç raw-—Ç–µ–∫—Å—Ç –≤ output/<org>/site_info.txt, –µ—Å–ª–∏ –æ–Ω –Ω–µ –ø—É—Å—Ç–æ–π;
    # 4) –æ—Ç–¥–∞—ë—Ç —Å—Ç—Ä—É–∫—Ç—É—Ä–∏—Ä–æ–≤–∞–Ω–Ω—É—é OrgInfo (–∏–ª–∏ ¬´–ø—É—Å—Ç—É—é¬ª, –µ—Å–ª–∏ —Å–∞–π—Ç–∞ –Ω–µ—Ç).
    # """
    # url = find_official_site(org)
    # if not url:
    #     logging.warning("Official site for %s not found", org)
    #     return OrgInfo([], [], [], [])
    #
    # console.print(f"[bold]–ß–∏—Ç–∞–µ–º –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç: {url}")
    # text = _diagnostic_download(url)            # ‚Üê –Ω–æ–≤—ã–π helper
    #
    # # ‚îÄ‚îÄ —Å–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ, –µ—Å–ª–∏ —Ö–æ—Ç—å —á—Ç–æ-—Ç–æ –∏–∑–≤–ª–µ–∫–ª–∏ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    # if text and len(text) >= 500:
    #     org_dir = OUTPUT_ROOT / org.replace(" ", "_")
    #     org_dir.mkdir(parents=True, exist_ok=True)
    #     (org_dir / "site_info.txt").write_text(text, encoding="utf-8")
    #     console.print(f"[green]üìù site_info.txt –∑–∞–ø–∏—Å–∞–Ω ({len(text)} —Å–∏–º–≤.)")
    # else:
    #     console.print("[yellow]‚ö† —Ç–µ–∫—Å—Ç –ø—É—Å—Ç ‚Äî —Ñ–∞–π–ª –Ω–µ —Å–æ–∑–¥–∞–Ω")
    #
    # return _extract_info(text)

def extract_official_info(org: str, out_dir: Path) -> OrgInfo:
    """
    1) –Ω–∞—Ö–æ–¥–∏—Ç –æ—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç,
    2) —Å–∫–∞—á–∏–≤–∞–µ—Ç –≥–ª–∞–≤–Ω—ã–π + –≤–Ω—É—Ç—Ä–µ–Ω–Ω–∏–µ —Å—Å—ã–ª–∫–∏ (1-–π —É—Ä–æ–≤–µ–Ω—å),
    3) —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç raw-—Ç–µ–∫—Å—Ç –≤ site_info.txt,
    4) –ø–∞—Ä—Å–∏—Ç LLM-–æ–º –≤ OrgInfo.
    """
    url = find_official_site(org)
    if not url:
        console.print("[yellow]‚ö† –û—Ñ–∏—Ü–∏–∞–ª—å–Ω—ã–π —Å–∞–π—Ç –Ω–µ –Ω–∞–π–¥–µ–Ω")
        return OrgInfo([], [], [], [])

    console.print(f"[bold]üåê –ö—Ä–∞—É–ª—é —Å–∞–π—Ç (1 —É—Ä–æ–≤–µ–Ω—å): {url}")
    text = crawl_one_level(url)
    if not text:
        console.print("[yellow]‚ö† –ù–µ—Ç –ø—Ä–∏–≥–æ–¥–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞")
        return OrgInfo([], [], [], [])

    (out_dir / "site_info.txt").write_text(text, encoding="utf-8")
    console.print(f"[green]üìù site_info.txt –∑–∞–ø–∏—Å–∞–Ω ({len(text)} —Å–∏–º–≤.)")

    return _extract_info(text)

# ---------------------------------------------------------------------------
# internet search
# ---------------------------------------------------------------------------

def gather_internet_info(org: str, max_results: int = 5) -> OrgInfo:
    """Search the web for public information about the organisation."""
    console.print("–ß–∏—Ç–∞–µ–º –∏–Ω—ã–µ –æ—Ç–∫—Ä—ã—Ç—ã–µ –∏—Å—Ç–æ—á–Ω–∏–∫–∏")

    texts: List[str] = []
    query = f"{org} —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 2022 2025"
    for url in search_duckduckgo(query, max_results=max_results):
        txt = fetch_text(url)
        if txt:
            texts.append(txt)
    return _extract_info("\n".join(texts))


# ---------------------------------------------------------------------------
# saving helpers
# ---------------------------------------------------------------------------

def save_json(info: OrgInfo, path: Path) -> None:
    path.write_text(json.dumps(asdict(info), ensure_ascii=False, indent=2), encoding="utf-8")
    console.print("—Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —Ñ–∞–π–ª")

def info_as_text(info: OrgInfo) -> str:
    lines = ["# –û—Å–Ω–æ–≤–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è –¥–µ—è—Ç–µ–ª—å–Ω–æ—Å—Ç–∏:"]
    lines += [f"- {a}" for a in info.activities]
    lines.append("\n# –û—Å–Ω–æ–≤–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã 2022-2025:")
    lines += [f"- {r}" for r in info.results]
    lines.append("\n# –ù–∞—É—á–Ω—ã–µ –Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–∏—è:")
    lines += [f"- {s}" for s in info.research]
    lines.append("\n# –ü–∞—Ä—Ç–Ω—ë—Ä—ã:")
    lines += [f"- {p}" for p in info.partners]
    return "\n".join(lines)


def save_txt(info: OrgInfo, path: Path) -> None:
    path.write_text(info_as_text(info), encoding="utf-8")


# ---------------------------------------------------------------------------
# public API
# ---------------------------------------------------------------------------

def discover_org(org: str, output_dir: Path) -> None:
    """Run discovery pipeline for the organisation."""
    console.print("–ó–∞–ø—É—Å—Ç–∏–ª–∏ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–æ–Ω–Ω—ã–π —Å–∫—Ä–∏–Ω–∏–Ω–≥ –æ—Ä–≥–∞–Ω–∏–∑–∞—Ü–∏–∏")
    output_dir.mkdir(parents=True, exist_ok=True)

    site_info = extract_official_info(org, output_dir)
    web_info = gather_internet_info(org)

    save_json(site_info, output_dir / "site_info.json")
    save_json(web_info, output_dir / "internet_info.json")

    save_txt(site_info, output_dir / "site_info.txt")
    save_txt(web_info, output_dir / "internet_info.txt")

def _diagnostic_download(url: str) -> str:
    """–°–∫–∞—á–∏–≤–∞–µ—Ç URL, –ø–æ–¥—Ä–æ–±–Ω–æ –ª–æ–≥–∏—Ä—É–µ—Ç —à–∞–≥–∏, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —á–∏—Å—Ç—ã–π —Ç–µ–∫—Å—Ç ('' –µ—Å–ª–∏ –Ω–µ—Ç)."""
    console.rule(f"[bold blue]üåê –°–∫–∞—á–∏–≤–∞–µ–º {url}")
    headers = {"User-Agent": FIREFOX_UA}

    # ‚îÄ‚îÄ 1. HTTP GET ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        r = requests.get(url, headers=headers, timeout=20)
        console.print(f"Status: {r.status_code}, bytes: {len(r.content)}")
        r.raise_for_status()
    except requests.RequestException as err:
        console.print(f"[red]HTTP error:[/] {err}")
        return ""

    # ‚îÄ‚îÄ 2. –ö–æ–¥–∏—Ä–æ–≤–∫–∞ ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    enc_before = r.encoding or "None"
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    console.print(f"Encoding: {enc_before} ‚Üí {r.encoding}")

    html = r.text

    # ‚îÄ‚îÄ 3. Trafilatura ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    console.print("‚Ä¢ Trafilatura.extract() ‚Ä¶")
    text = trafilatura.extract(
        html,
        include_images=False,
        include_tables=False,
        no_fallback=False,
        target_language="ru",
    )
    if text:
        console.print(f"[green]‚úî Trafilatura OK:[/] {len(text)} chars")
        return text

    console.print("[yellow]Trafilatura –≤–µ—Ä–Ω—É–ª–∞ None ‚Äì –ø—Ä–æ–±—É–µ–º fallback Readability")

    # ‚îÄ‚îÄ 4. Readability fallback ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ
    try:
        doc = Document(html)
        text = BS(doc.summary(), "lxml").get_text(" ", strip=True)
        console.print(f"[green]‚úî Readability OK:[/] {len(text)} chars")
        return text
    except Exception as err:
        console.print(f"[red]Readability failed:[/] {err}")
        return ""

def crawl_one_level(start_url: str, max_pages: int = 10, min_len: int = 400) -> str:
    """
    –°–∫–∞—á–∏–≤–∞–µ—Ç –≥–ª–∞–≤–Ω—É—é —Å—Ç—Ä–∞–Ω–∏—Ü—É + –≤—Å–µ —Å—Å—ã–ª–∫–∏ 1-–≥–æ —É—Ä–æ–≤–Ω—è –≤–Ω—É—Ç—Ä–∏ —Ç–æ–≥–æ –∂–µ –¥–æ–º–µ–Ω–∞.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–±—ä–µ–¥–∏–Ω—ë–Ω–Ω—ã–π ¬´—á–∏—Å—Ç—ã–π¬ª —Ç–µ–∫—Å—Ç (–∏–ª–∏ '' –µ—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ—Ç).
    """
    domain = urlparse(start_url).netloc
    visited: set[str] = set()
    queue   : list[str] = [start_url]
    texts   : list[str] = []

    while queue and len(visited) < max_pages:
        url = queue.pop(0)
        if url in visited:
            continue
        visited.add(url)

        try:
            r = requests.get(url, headers=HEADERS, timeout=15)
            r.raise_for_status()
        except requests.RequestException:
            continue

        html = r.text
        txt = trafilatura.extract(html, target_language="ru", no_fallback=False) or ""
        if len(txt) >= min_len:
            texts.append(txt)

        # —Å–æ–±–∏—Ä–∞–µ–º —Å—Å—ã–ª–∫–∏ –≥–ª—É–±–∏–Ω—ã 1
        soup = BS(html, "lxml")
        for a in soup.find_all("a", href=True):
            link = urljoin(url, a["href"])
            if urlparse(link).netloc == domain and link not in visited:
                queue.append(link)

        time.sleep(0.5 + random.uniform(0, 0.5))   # –±–µ—Ä–µ–∂—ë–º —Å–µ—Ä–≤–µ—Ä

    return "\n".join(texts)