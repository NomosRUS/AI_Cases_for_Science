"""Utilities for discovering information about an organisation.

The module performs five operations:
1. Find the official web site of an organisation by name.
2. Extract key facts from the official web site: activities, 2022-2025 results,
   research directions and partners.
3. Search the internet and collect the same information from public sources.
4. Store data from steps 2 and 3 in separate JSON files.
5. Save human readable summaries from steps 2 and 3 in two txt files.
"""



from __future__ import annotations
import itertools


from rich import box, table
from readability import Document
from bs4 import BeautifulSoup as BS

from selenium import webdriver
from selenium.webdriver.firefox.options import Options
from selenium.webdriver.common.by import By
from selenium.webdriver.firefox.service import Service
from webdriver_manager.firefox import GeckoDriverManager

from urllib.parse import quote_plus
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.common.exceptions import TimeoutException


from fake_useragent import UserAgent
import json
import logging
import os
import urllib.parse
from dataclasses import dataclass, asdict
from pathlib import Path

import re, urllib.parse, tldextract, requests
from transliterate import translit
from langchain_openai import ChatOpenAI
from langchain.prompts import ChatPromptTemplate
import requests_cache
import trafilatura
from .utils import extract_json
from typing import Sequence

# cache web requests to speed up repeated runs
requests_cache.install_cache("ai_scout_cache")

import time
import random
from typing import List
from duckduckgo_search import DDGS
from duckduckgo_search.exceptions import DuckDuckGoSearchException
from rich.console import Console

if os.getenv("OPENAI_API_KEY"):
    print("OPENAI_API_KEY Ğ½Ğ°ÑÑ‚Ñ€Ğ¾ĞµĞ½Ğ°")
else:
    print("ĞŸĞµÑ€ĞµĞ¼ĞµĞ½Ğ½Ğ°Ñ OPENAI_API_KEY Ğ½Ğµ Ğ·Ğ°Ğ´Ğ°Ğ½Ğ°")

console = Console()

# Ğ ĞµĞ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Firefox UA (Ğ¸ÑĞ½ÑŒ-2025)
FIREFOX_UA = (
    "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:126.0) "
    "Gecko/20100101 Firefox/126.0"
)
HEADERS = {
    "User-Agent": FIREFOX_UA,
    "Accept":
        "text/html,application/xhtml+xml,"
        "application/xml;q=0.9,image/avif,image/webp,*/*;q=0.8",
    "Accept-Language": "ru,en;q=0.9",
    "DNT": "1",
}
OUTPUT_ROOT = Path("output")
GOOD_TLDS = {"ru", "su", "org", "edu", "ac", "science", "tech"}
_MAX_RETRIES = 3          # ÑĞºĞ¾Ğ»ÑŒĞºĞ¾ Ñ€Ğ°Ğ· Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ Ğ¿Ñ€ĞµĞ¶Ğ´Ğµ Ñ‡ĞµĞ¼ ÑĞ´Ğ°Ñ‚ÑŒÑÑ
_BASE_SLEEP  = 2          # Ğ±Ğ°Ğ·Ğ¾Ğ²Ğ°Ñ Ğ·Ğ°Ğ´ĞµÑ€Ğ¶ĞºĞ° (ÑĞµĞº)


@dataclass
class OrgInfo:
    """Structured information about an organisation."""

    activities: List[str]
    results: List[str]
    research: List[str]
    partners: List[str]


PROMPT_INFO = """
Ğ¢Ñ‹ Ğ°ÑÑĞ¾Ñ†Ğ¸Ğ¸Ñ€ÑƒĞµÑˆÑŒÑÑ Ñ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ñ‚Ğ¸ĞºĞ¾Ğ¹. ĞŸÑ€Ğ¾Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ¸Ñ€ÑƒĞ¹ Ñ‚ĞµĞºÑÑ‚ {text} Ğ¸
Ğ²Ñ‹Ğ´ĞµĞ»Ğ¸:
1. Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸,
2. Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ·Ğ° 2022-2025 Ğ³Ğ¾Ğ´Ñ‹,
3. Ğ¾ÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸,
4. ĞºĞ»ÑÑ‡ĞµĞ²Ñ‹Ñ… Ğ¿Ğ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ğ¾Ğ² Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸.
Ğ’ĞµÑ€Ğ½Ğ¸ **ÑÑ‚Ñ€Ğ¾Ğ³Ğ¾ JSON**:
{{"activities": [...], "results": [...], "research": [...], "partners": [...]}}
"""


# ---------------------------------------------------------------------------
# generic helpers
# ---------------------------------------------------------------------------


def search_duckduckgo(query: str, max_results: int = 2) -> List[str]:
    """DuckDuckGo search with Firefox UA, back-off and verbose logging."""
    console.print(f"[cyan]â†’ DuckDuckGo query:[/] {query}")

    for attempt in range(3):                    # â‰¤ 3 Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸
        try:
            with DDGS(headers=HEADERS, timeout=15) as ddgs:
                hits = [
                    r["href"] for r in ddgs.text(query, max_results=max_results)
                    if r.get("href")
                ]

            if hits:
                console.print(
                    "[green]âœ” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:[/]\n  " + "\n  ".join(hits[:2])
                )
            else:
                console.print("[yellow]âš  Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾[/]")

            return hits

        except DuckDuckGoSearchException as err:
            wait = (2 ** attempt) + random.uniform(0, 1.2)
            console.print(
                f"[yellow]âš  Rate-limit:[/] {err}. "
                f"ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€ Ñ‡ĞµÑ€ĞµĞ· {wait:0.1f} Ñ."
            )
            time.sleep(wait)

    console.print("[red]âŒ DuckDuckGo: Ğ²ÑĞµ Ğ¿Ğ¾Ğ¿Ñ‹Ñ‚ĞºĞ¸ Ğ¸ÑÑ‡ĞµÑ€Ğ¿Ğ°Ğ½Ñ‹[/]")
    return []

def ddg_first_links_firefox(query: str, n: int = 3) -> list[str]:
    """
    Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ğ¿ĞµÑ€Ğ²Ñ‹Ğµ n ÑÑÑ‹Ğ»Ğ¾Ğº DuckDuckGo Ñ‡ĞµÑ€ĞµĞ· headless-Firefox.
    â€¢ ĞĞµ ĞºĞ»Ğ¸ĞºĞ°ĞµÑ‚ Ñ„Ğ¾Ñ€Ğ¼Ñƒ; ÑÑ€Ğ°Ğ·Ñƒ Ğ¾Ñ‚ĞºÑ€Ñ‹Ğ²Ğ°ĞµÑ‚ URL Ğ²Ğ¸Ğ´Ğ° `/?q=...&ia=web`.
    â€¢ Ğ–Ğ´Ñ‘Ñ‚ Ğ´Ğ¾ 7 Ñ Ğ¿Ğ¾ÑĞ²Ğ»ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ² Ğ¸ Ğ±ĞµÑ€Ñ‘Ñ‚ ÑÑÑ‹Ğ»ĞºĞ¸ Ğ¿Ğ¾ CSS `.result__a`.
    """
    console.print(f"[cyan]â†’ Firefox DDG query:[/] {query}")

    options = Options()
    options.headless = True
    driver = webdriver.Firefox(
        service=Service(GeckoDriverManager().install()),
        options=options,
    )

    try:
        url = (
            "https://duckduckgo.com/?q="
            + quote_plus(query)
            + "&ia=web&kl=ru-ru"
        )
        driver.get(url)

        try:
            WebDriverWait(driver, 7).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "a.result__a"))
            )
        except TimeoutException:
            console.print("[yellow]âš  DDG: Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ Ğ½Ğµ Ğ¿Ğ¾ÑĞ²Ğ¸Ğ»Ğ¸ÑÑŒ Ğ·Ğ° 7 Ñ[/]")
            return []

        links = driver.find_elements(By.CSS_SELECTOR, "a.result__a")[: n]
        hrefs = [link.get_attribute("href") for link in links]

        if hrefs:
            console.print(
                "[green]âœ” Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹:[/]\n  " + "\n  ".join(hrefs)
            )
        else:
            console.print("[yellow]âš  Ğ½Ğ¸Ñ‡ĞµĞ³Ğ¾ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ğ¾[/]")

        return hrefs

    finally:
        driver.quit()


def fetch_text(url: str) -> str:
    """Download and clean page text."""

    try:
        downloaded = trafilatura.fetch_url(url)
        if downloaded:
            return trafilatura.extract(downloaded) or ""
    except Exception as exc:  # noqa: BLE001
        logging.warning("Failed to fetch %s: %s", url, exc)
    return ""


# ---------------------------------------------------------------------------
# information extraction
# ---------------------------------------------------------------------------

def _extract_info(text: str, model: str = "gpt-4o-mini") -> OrgInfo:
    """
    Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµÑ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾ Ğ½Ğ°ÑƒÑ‡Ğ½Ğ¾Ğ¹ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸ Ñ Ğ¿Ğ¾Ğ¼Ğ¾Ñ‰ÑŒÑ LLM.

    Args:
        text: ÑÑ‹Ñ€Ğ¾Ğ¹ Ğ¾Ğ±ÑŠĞµĞ´Ğ¸Ğ½Ñ‘Ğ½Ğ½Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ (About, Ğ½Ğ¾Ğ²Ğ¾ÑÑ‚Ğ¸, Ğ¿ÑƒĞ±Ğ»Ğ¸ĞºĞ°Ñ†Ğ¸Ğ¸).
        model: Ğ¸Ğ¼Ñ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸ OpenAI Chat-Completion API (Ğ¿Ğ¾ ÑƒĞ¼Ğ¾Ğ»Ñ‡Ğ°Ğ½Ğ¸Ñ gpt-4o-mini).

    Returns:
        OrgInfo dataclass Ñ Ñ‡ĞµÑ‚Ñ‹Ñ€ÑŒĞ¼Ñ ÑĞ¿Ğ¸ÑĞºĞ°Ğ¼Ğ¸ ÑÑ‚Ñ€Ğ¾Ğº.
    """
    console.print("[bold cyan]â³  Ğ˜Ğ·Ğ²Ğ»ĞµĞºĞ°ĞµĞ¼ ĞºĞ»ÑÑ‡ĞµĞ²ÑƒÑ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· Ñ‚ĞµĞºÑÑ‚Ğ°â€¦")

    # 1. Ğ¡Ğ¾Ğ·Ğ´Ğ°Ñ‘Ğ¼ Ñ‡Ğ°Ñ‚-LLM
    llm = ChatOpenAI(
        model=model,
        temperature=0,
        openai_api_key=os.getenv("OPENAI_API_KEY"),  # Ğ±ĞµĞ· Ğ´ĞµÑ„Ğ¾Ğ»Ñ‚Ğ°-Ğ¿Ğ»Ğ°Ñ†ĞµĞ±Ğ¾!
    )

    # 2. Ğ“Ğ¾Ñ‚Ğ¾Ğ²Ğ¸Ğ¼ Ñ‡Ğ°Ñ‚-ÑˆĞ°Ğ±Ğ»Ğ¾Ğ½
    prompt = ChatPromptTemplate.from_template(PROMPT_INFO)
    print(prompt.format(text="demo"))

    # 3. Ğ—Ğ°Ğ¿ÑƒÑĞºĞ°ĞµĞ¼ Ñ†ĞµĞ¿Ğ¾Ñ‡ĞºÑƒ (Prompt â†’ ChatOpenAI)
    chain = prompt | llm
    result = chain.invoke({"text": text[:4000]})  # â‰¤4 000 ÑĞ¸Ğ¼Ğ²Ğ¾Ğ»Ğ¾Ğ² Ğ´Ğ»Ñ ÑĞºĞ¾Ğ½Ğ¾Ğ¼Ğ¸Ğ¸

    # 4. JSON-Ğ¿Ğ°Ñ€ÑĞ¸Ğ½Ğ³ + Ñ„Ğ¾Ñ€Ğ¼Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ¸Ğµ dataclass
    data = extract_json(result)
    return OrgInfo(
        activities=data.get("activities", []),
        results=data.get("results", []),
        research=data.get("research", []),
        partners=data.get("partners", []),
    )

# ---------------------------------------------------------------------------
# official web site
# ---------------------------------------------------------------------------

def _clean_name(name: str) -> str:
    """Ğ£Ğ±Ğ¸Ñ€Ğ°ĞµĞ¼ ÑÑ€.Ñ„Ğ¾Ñ€Ğ¼Ñ‹ / ĞºĞ°Ğ²Ñ‹Ñ‡ĞºĞ¸, Ğ¾ÑÑ‚Ğ°Ğ²Ğ»ÑÑ Â«ÑÑƒÑ‚ÑŒÂ»."""
    return re.sub(
        r"(Ğ¤Ğ“Ğ‘Ğ£|Ğ¤Ğ˜Ğ¦|ĞĞ˜Ğ˜|ĞĞ|Ğ¤Ğ“Ğ£ĞŸ|Ğ¸Ğ¼\..*|\".*?\"|[Â«Â»])",
        "",
        name,
        flags=re.I
    ).strip()


def _looks_like_official(url: str, clean: str, abbr: str) -> bool:
    """Ğ­Ğ²Ñ€Ğ¸ÑÑ‚Ğ¸ĞºĞ°: Ğ´Ğ¾Ğ¼ĞµĞ½ Ğ½Ğµ wiki/ÑĞ¿Ñ€Ğ°Ğ²Ğ¾Ñ‡Ğ½Ğ¸Ğº + ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ¸Ñ‚ Ğ°Ğ±Ğ±Ñ€ĞµĞ²Ğ¸Ğ°Ñ‚ÑƒÑ€Ñƒ
    Ğ¸Ğ»Ğ¸ Ğ»ÑĞ±ÑƒÑ Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ Ñ‡Ğ°ÑÑ‚ÑŒ Ğ½Ğ°Ğ·Ğ²Ğ°Ğ½Ğ¸Ñ."""
    host = urllib.parse.urlparse(url).netloc.lower()

    if host.startswith("ru.wikipedia.org") or host.endswith(".wikipedia.org"):
        return False

    # Ğ»Ğ°Ñ‚Ğ¸Ğ½Ğ¸Ğ·Ğ¸Ñ€ÑƒĞµĞ¼ Ğ¿ĞµÑ€Ğ²Ğ¾Ğµ ÑĞ¾Ğ´ĞµÑ€Ğ¶Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾Ğµ ÑĞ»Ğ¾Ğ²Ğ¾ (Institute â†’ institute, Ğ˜Ğ½ÑÑ‚Ğ¸Ñ‚ÑƒÑ‚ â†’ institut)
    first_word = translit(clean.split()[0], 'ru', reversed=True).lower()

    return (
        first_word in host
        or abbr.lower() in host              # Ğ ĞĞ â†’ ran, Ğ˜ĞĞ¥ â†’ inh Ğ¸ Ñ‚.Ğ¿.
        or "ras." in host                    # Ğ±Ğ¾Ğ»ÑŒÑˆĞ¸Ğ½ÑÑ‚Ğ²Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ¾Ğ² Ğ ĞĞ
    )


def find_official_site(org: str) -> str:
    """Ğ’Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ URL Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ° Ğ»Ğ¸Ğ±Ğ¾ ''."""
    console.rule("[bold green]ğŸ” ĞŸĞ¾Ğ¸ÑĞº Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ ÑĞ°Ğ¹Ñ‚Ğ°")
    clean = _clean_name(org)
    abbr  = "".join(w[0] for w in clean.split() if len(w) > 2)

    queries = [
        f"{clean} Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚",
        f"{clean} ÑĞ°Ğ¹Ñ‚",
        f"{translit(clean, 'ru', reversed=True)} official website",
        f"{abbr} ÑĞ°Ğ¹Ñ‚" if len(abbr) > 3 else "",
    ]

    for q in queries:
        if not q:
            continue

        # ĞŸĞ Ğ˜ĞĞ Ğ˜Ğ¢Ğ•Ğ¢ â€” headless Firefox (Ğ±ĞµĞ· ratelimit)
        for url in ddg_first_links_firefox(q, n=4):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]âœ” Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½:[/] {url}")
                return url

        # fallback Ğ½Ğ° duckduckgo_search (Ğ±Ñ‹ÑÑ‚Ñ€ĞµĞµ, Ğ½Ğ¾ Ñ Ñ€Ğ¸ÑĞºĞ¾Ğ¼ 202)
        for url in search_duckduckgo(q, max_results=2):
            if _looks_like_official(url, clean, abbr):
                console.print(f"[green]âœ” Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½:[/] {url}")
                return url

        time.sleep(1)  # Ñ‡ĞµÑÑ‚Ğ½Ğ°Ñ Ğ¿Ğ°ÑƒĞ·Ğ° Ğ¿ĞµÑ€ĞµĞ´ ÑĞ»ĞµĞ´ÑƒÑÑ‰ĞµĞ¹ Ñ„Ñ€Ğ°Ğ·Ğ¾Ğ¹

    console.print("[yellow]âš  Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½")
    return ""



def extract_official_info(org: str) -> OrgInfo:
    """
    1) Ğ¸Ñ‰ĞµÑ‚ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚;
    2) ÑĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ Ğ¸ Ñ‡Ğ¸ÑÑ‚Ğ¸Ñ‚ Ñ‚ĞµĞºÑÑ‚ (Ñ‡ĞµÑ€ĞµĞ· _diagnostic_download);
    3) Ğ¿Ğ¸ÑˆĞµÑ‚ raw-Ñ‚ĞµĞºÑÑ‚ Ğ² output/<org>/site_info.txt, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ Ğ½Ğµ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹;
    4) Ğ¾Ñ‚Ğ´Ğ°Ñ‘Ñ‚ ÑÑ‚Ñ€ÑƒĞºÑ‚ÑƒÑ€Ğ¸Ñ€Ğ¾Ğ²Ğ°Ğ½Ğ½ÑƒÑ OrgInfo (Ğ¸Ğ»Ğ¸ Â«Ğ¿ÑƒÑÑ‚ÑƒÑÂ», ĞµÑĞ»Ğ¸ ÑĞ°Ğ¹Ñ‚Ğ° Ğ½ĞµÑ‚).
    """
    url = find_official_site(org)
    if not url:
        logging.warning("Official site for %s not found", org)
        return OrgInfo([], [], [], [])

    console.print(f"[bold]Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¾Ñ„Ğ¸Ñ†Ğ¸Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ ÑĞ°Ğ¹Ñ‚: {url}")
    text = _diagnostic_download(url)            # â† Ğ½Ğ¾Ğ²Ñ‹Ğ¹ helper

    # â”€â”€ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ÑĞµĞ¼ Ñ‚Ğ¾Ğ»ÑŒĞºĞ¾, ĞµÑĞ»Ğ¸ Ñ…Ğ¾Ñ‚ÑŒ Ñ‡Ñ‚Ğ¾-Ñ‚Ğ¾ Ğ¸Ğ·Ğ²Ğ»ĞµĞºĞ»Ğ¸ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    if text and len(text) >= 500:
        org_dir = OUTPUT_ROOT / org.replace(" ", "_")
        org_dir.mkdir(parents=True, exist_ok=True)
        (org_dir / "site_info.txt").write_text(text, encoding="utf-8")
        console.print(f"[green]ğŸ“ site_info.txt Ğ·Ğ°Ğ¿Ğ¸ÑĞ°Ğ½ ({len(text)} ÑĞ¸Ğ¼Ğ².)")
    else:
        console.print("[yellow]âš  Ñ‚ĞµĞºÑÑ‚ Ğ¿ÑƒÑÑ‚ â€” Ñ„Ğ°Ğ¹Ğ» Ğ½Ğµ ÑĞ¾Ğ·Ğ´Ğ°Ğ½")

    return _extract_info(text)

# ---------------------------------------------------------------------------
# internet search
# ---------------------------------------------------------------------------

def gather_internet_info(org: str, max_results: int = 5) -> OrgInfo:
    """Search the web for public information about the organisation."""
    console.print("Ğ§Ğ¸Ñ‚Ğ°ĞµĞ¼ Ğ¸Ğ½Ñ‹Ğµ Ğ¾Ñ‚ĞºÑ€Ñ‹Ñ‚Ñ‹Ğµ Ğ¸ÑÑ‚Ğ¾Ñ‡Ğ½Ğ¸ĞºĞ¸")

    texts: List[str] = []
    query = f"{org} Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 2022 2025"
    for url in search_duckduckgo(query, max_results=max_results):
        txt = fetch_text(url)
        if txt:
            texts.append(txt)
    return _extract_info("\n".join(texts))


# ---------------------------------------------------------------------------
# saving helpers
# ---------------------------------------------------------------------------

def save_json(info: OrgInfo, path: Path) -> None:
    path.write_text(json.dumps(asdict(info), ensure_ascii=False, indent=2), encoding="utf-8")
    console.print("ÑĞ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ğ»Ğ¸ Ñ„Ğ°Ğ¹Ğ»")

def info_as_text(info: OrgInfo) -> str:
    lines = ["# ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ Ğ´ĞµÑÑ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚Ğ¸:"]
    lines += [f"- {a}" for a in info.activities]
    lines.append("\n# ĞÑĞ½Ğ¾Ğ²Ğ½Ñ‹Ğµ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ 2022-2025:")
    lines += [f"- {r}" for r in info.results]
    lines.append("\n# ĞĞ°ÑƒÑ‡Ğ½Ñ‹Ğµ Ğ½Ğ°Ğ¿Ñ€Ğ°Ğ²Ğ»ĞµĞ½Ğ¸Ñ:")
    lines += [f"- {s}" for s in info.research]
    lines.append("\n# ĞŸĞ°Ñ€Ñ‚Ğ½Ñ‘Ñ€Ñ‹:")
    lines += [f"- {p}" for p in info.partners]
    return "\n".join(lines)


def save_txt(info: OrgInfo, path: Path) -> None:
    path.write_text(info_as_text(info), encoding="utf-8")


# ---------------------------------------------------------------------------
# public API
# ---------------------------------------------------------------------------

def discover_org(org: str, output_dir: Path) -> None:
    """Run discovery pipeline for the organisation."""
    console.print("Ğ—Ğ°Ğ¿ÑƒÑÑ‚Ğ¸Ğ»Ğ¸ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ğ¾Ğ½Ğ½Ñ‹Ğ¹ ÑĞºÑ€Ğ¸Ğ½Ğ¸Ğ½Ğ³ Ğ¾Ñ€Ğ³Ğ°Ğ½Ğ¸Ğ·Ğ°Ñ†Ğ¸Ğ¸")
    output_dir.mkdir(parents=True, exist_ok=True)

    site_info = extract_official_info(org)
    web_info = gather_internet_info(org)

    save_json(site_info, output_dir / "site_info.json")
    save_json(web_info, output_dir / "internet_info.json")

    save_txt(site_info, output_dir / "site_info.txt")
    save_txt(web_info, output_dir / "internet_info.txt")

def _diagnostic_download(url: str) -> str:
    """Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµÑ‚ URL, Ğ¿Ğ¾Ğ´Ñ€Ğ¾Ğ±Ğ½Ğ¾ Ğ»Ğ¾Ğ³Ğ¸Ñ€ÑƒĞµÑ‚ ÑˆĞ°Ğ³Ğ¸, Ğ²Ğ¾Ğ·Ğ²Ñ€Ğ°Ñ‰Ğ°ĞµÑ‚ Ñ‡Ğ¸ÑÑ‚Ñ‹Ğ¹ Ñ‚ĞµĞºÑÑ‚ ('' ĞµÑĞ»Ğ¸ Ğ½ĞµÑ‚)."""
    console.rule(f"[bold blue]ğŸŒ Ğ¡ĞºĞ°Ñ‡Ğ¸Ğ²Ğ°ĞµĞ¼ {url}")
    headers = {"User-Agent": FIREFOX_UA}

    # â”€â”€ 1. HTTP GET â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        r = requests.get(url, headers=headers, timeout=20)
        console.print(f"Status: {r.status_code}, bytes: {len(r.content)}")
        r.raise_for_status()
    except requests.RequestException as err:
        console.print(f"[red]HTTP error:[/] {err}")
        return ""

    # â”€â”€ 2. ĞšĞ¾Ğ´Ğ¸Ñ€Ğ¾Ğ²ĞºĞ° â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    enc_before = r.encoding or "None"
    if not r.encoding or r.encoding.lower() == "iso-8859-1":
        r.encoding = r.apparent_encoding
    console.print(f"Encoding: {enc_before} â†’ {r.encoding}")

    html = r.text

    # â”€â”€ 3. Trafilatura â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    console.print("â€¢ Trafilatura.extract() â€¦")
    text = trafilatura.extract(
        html,
        include_images=False,
        include_tables=False,
        no_fallback=False,
        target_language="ru",
    )
    if text:
        console.print(f"[green]âœ” Trafilatura OK:[/] {len(text)} chars")
        return text

    console.print("[yellow]Trafilatura Ğ²ĞµÑ€Ğ½ÑƒĞ»Ğ° None â€“ Ğ¿Ñ€Ğ¾Ğ±ÑƒĞµĞ¼ fallback Readability")

    # â”€â”€ 4. Readability fallback â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    try:
        doc = Document(html)
        text = BS(doc.summary(), "lxml").get_text(" ", strip=True)
        console.print(f"[green]âœ” Readability OK:[/] {len(text)} chars")
        return text
    except Exception as err:
        console.print(f"[red]Readability failed:[/] {err}")
        return ""